{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ed002c-398c-4995-8783-49444fd3bcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/workdir/unsupervised_pretrain/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba638b32-ecd0-414a-b6a9-75a4d54724db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from models import SeriesResNet18\n",
    "from datasets import SeriesEmbedDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1de1d7-60c3-4add-bd58-135680abc24b",
   "metadata": {},
   "source": [
    "# Get ready to do some business #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067f2ba7-4ec6-410b-a631-37b686946afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = SeriesEmbedDataset([\"/datasets/datasets/unsupervised-sentinel2/testset-16SEF/\"], size=512, series_length=20, bands=[2,3,4,8,9])\n",
    "ds = SeriesEmbedDataset([\"/datasets/datasets/unsupervised-sentinel2/testset-16SEF/\"], size=512, series_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a882a01-8af4-4a60-9087-e03d1b310f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = SeriesEmbedDataset([\"/datasets/datasets/berlin/32UQD/\"], size=512, series_length=8)\n",
    "# print(len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe7538-02de-4a07-946b-cbf768bdd330",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    ds,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8f2ef8-729a-4f9a-a1a2-c13cfd2f89ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load the test set, compute embeddings, save embeddings #\n",
    "\n",
    "This only needs to be done once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98340e36-ca2a-43b8-8d12-18a9ab932fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b42637-ba94-476a-977c-a899b6570c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"/workdir/unsupervised_pretrain/12band-resnet34.pth\", map_location=device).to(device)\n",
    "model = model.eval()\n",
    "autoencoder = torch.load(\"/workdir/unsupervised_pretrain/12band-resnet34-autoencoder.pth\", map_location=device).to(device)\n",
    "autoencoder = autoencoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609d5400-2eb1-47d3-9c6b-790e613e5f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_embeddings = []\n",
    "text_embeddings = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for imagery, _, text_embedding in tqdm(dataloader):\n",
    "        visual_embedding = model(imagery.to(device))\n",
    "        visual_embedding = F.normalize(visual_embedding, dim=1)\n",
    "        text_embedding = F.normalize(text_embedding.to(device), dim=1)\n",
    "\n",
    "        visual_embedding = visual_embedding.detach().cpu()\n",
    "        text_embedding = text_embedding.detach().cpu()\n",
    "\n",
    "        visual_embeddings.append(visual_embedding)\n",
    "        text_embeddings.append(text_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8872a54-aa66-44e8-a109-5e40f09f46cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = torch.cat(text_embeddings, dim=0)\n",
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d2c98-9078-48d8-9d10-f5d9d7405b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(text_embeddings, \"/workdir/unsupervised_pretrain/jupyter/text-embeddings.t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1acc7b1-2fb0-4b35-a380-237e37c4c794",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_embeddings = torch.cat(visual_embeddings, dim=0)\n",
    "visual_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cec020-6edf-4410-8892-c7fcaf65442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(visual_embeddings, \"/workdir/unsupervised_pretrain/jupyter/visual-embeddings.t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdc6052-807a-452a-a3b8-6edb31699acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    stuff = autoencoder(F.normalize(visual_embeddings.to(device), dim=1), text_embeddings.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef1f50-3deb-49a8-83c1-da1c67df3ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(stuff, \"/workdir/unsupervised_pretrain/jupyter/autoencoder-output.t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ead9a8-c851-4eb2-a34f-115dfff812c3",
   "metadata": {},
   "source": [
    "# Load embeddings #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478332d3-286e-43c3-9b85-f24a60d87772",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56eaec3-cdf7-434c-aa9d-5ea1e69688ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"/workdir/unsupervised_pretrain/12band-resnet34.pth\", map_location=device).to(device)\n",
    "model = model.eval()\n",
    "autoencoder = torch.load(\"/workdir/unsupervised_pretrain/12band-resnet34-autoencoder.pth\", map_location=device).to(device)\n",
    "autoencoder = autoencoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be43e1d7-9ff7-4417-a78d-bb4380518e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "_text_embeddings = torch.load(\"/workdir/unsupervised_pretrain/jupyter/text-embeddings.t\")\n",
    "text_embeddings = _text_embeddings.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcedc82d-5bc7-4d4b-9c9d-c0abf67cbc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "_visual_embeddings = torch.load(\"/workdir/unsupervised_pretrain/jupyter/visual-embeddings.t\")\n",
    "visual_embeddings = _visual_embeddings.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b27229-e2e6-43f4-816e-3c20310afb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_stuff = torch.load(\"/workdir/unsupervised_pretrain/jupyter/autoencoder-output.t\")\n",
    "stuff = [thing.detach().cpu().numpy() for thing in _stuff]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11536667-8ce1-4c9a-a6b6-58fff105fbae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2D ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bacb01-f4d1-4cc1-95f3-14cba35a4962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd22425a-5701-4fe5-8a7e-6619c029df0f",
   "metadata": {},
   "source": [
    "### Visual embeddings ###\n",
    "\n",
    "Blue dots are (projections) of original embeddings, orange dots are reconstructed by/from the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8b5bc1-ea87-4002-a44d-16045532c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "\n",
    "data0 = tsne.fit_transform(visual_embeddings)\n",
    "data1 = tsne.fit_transform(stuff[0])\n",
    "\n",
    "# plot the result\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(data0[:, 0], data0[:, 1])\n",
    "plt.scatter(data1[:, 0], data1[:, 1])\n",
    "# plt.scatter(data_2d[[333], 0], data_2d[[333], 1])  # Wood\n",
    "# plt.scatter(data_2d[[82], 0], data_2d[[82], 1])  # City\n",
    "# plt.scatter(data_2d[[440], 0], data_2d[[440], 1])  # Water\n",
    "plt.xlabel(\"t-SNE feature 0\")\n",
    "plt.ylabel(\"t-SNE feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee06662-a612-4b93-a757-ae193d459ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.abs(np.mean(visual_embeddings, axis=1))), np.max(np.abs(np.mean(stuff[0], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e66dfa3-28b5-47e0-adc2-9803ce3d95c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.abs(np.mean(visual_embeddings - stuff[0], axis=1))), np.mean(np.abs(np.mean(visual_embeddings - stuff[0], axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d616796e-cf18-46ff-b9ab-2e31c24709a2",
   "metadata": {},
   "source": [
    "### Text embeddings ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8011441d-1746-4899-871d-91404dcef032",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "\n",
    "mask = ~np.isnan(text_embeddings[:, 0])\n",
    "data0 = tsne.fit_transform(text_embeddings[mask])\n",
    "data1 = tsne.fit_transform(stuff[1][mask])\n",
    "\n",
    "# plot the result\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(data0[:, 0], data0[:, 1])\n",
    "plt.scatter(data1[:, 0], data1[:, 1])\n",
    "# plt.scatter(data_2d[[333], 0], data_2d[[333], 1])  # Wood\n",
    "# plt.scatter(data_2d[[82], 0], data_2d[[82], 1])  # City\n",
    "# plt.scatter(data_2d[[440], 0], data_2d[[440], 1])  # Water\n",
    "plt.xlabel(\"t-SNE feature 0\")\n",
    "plt.ylabel(\"t-SNE feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b1397c-0c5a-4213-8cd3-f4e5ea627227",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~np.isnan(text_embeddings[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6919d4d-2f8f-4399-bf64-800979cd4145",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.abs(np.mean(text_embeddings[mask], axis=1))), np.max(np.abs(np.mean(stuff[1][mask], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd268b18-bb8d-47d3-a444-c17dc4c9bf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.abs(np.mean(text_embeddings[mask] - stuff[1][mask], axis=1))), np.mean(np.abs(np.mean(text_embeddings[mask] - stuff[1][mask], axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6410746-01d4-4a52-b051-a400ade73717",
   "metadata": {},
   "source": [
    "### Shared latent space ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43dcc21-ec33-43f1-bc9f-54e7e9b5ded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "\n",
    "mask = ~np.isnan(text_embeddings[:, 0])\n",
    "data0 = tsne.fit_transform(stuff[2][mask])\n",
    "data1 = tsne.fit_transform(stuff[3][mask])\n",
    "\n",
    "# plot the result\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(data0[:, 0], data0[:, 1])\n",
    "plt.scatter(data1[:, 0], data1[:, 1])\n",
    "# plt.scatter(data_2d[[333], 0], data_2d[[333], 1])  # Wood\n",
    "# plt.scatter(data_2d[[82], 0], data_2d[[82], 1])  # City\n",
    "# plt.scatter(data_2d[[440], 0], data_2d[[440], 1])  # Water\n",
    "plt.xlabel(\"t-SNE feature 0\")\n",
    "plt.ylabel(\"t-SNE feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d85c3e4-1321-4423-ab9f-d8ccec37f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~np.isnan(text_embeddings[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055dc233-421b-46d6-9105-a2a8add2600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.abs(np.mean(stuff[2][mask], axis=1))), np.max(np.abs(np.mean(stuff[3][mask], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab49ded9-2428-403e-97b0-94b2391158ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.abs(np.mean(stuff[2][mask] - stuff[3][mask], axis=1))), np.mean(np.abs(np.mean(stuff[2][mask] - stuff[3][mask], axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9206a18-c8a1-4034-b45a-9a5a2da480a6",
   "metadata": {},
   "source": [
    "# Look for similarity #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39e7ff4-b9f9-42ea-b29a-7ac649ed952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccc420a-429a-4850-84b5-6eac1cfbf68e",
   "metadata": {},
   "source": [
    "## Utility functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5ba2a9-4243-4dcb-b49d-63fe6009d6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_query_cosine(query_vector, data, k, skip=2):\n",
    "    # calculate cosine similarities\n",
    "    cosine_similarities = cosine_similarity(data, query_vector.reshape(1, -1)).flatten()\n",
    "\n",
    "    # get top-k indices\n",
    "    k_skip = k * skip\n",
    "    top_k_indices = np.argpartition(-cosine_similarities, k_skip)[:k_skip]\n",
    "    top_k_indices = list(filter(lambda n: n % skip == 0, top_k_indices))\n",
    "\n",
    "    # return indices of the top-k closest vectors\n",
    "    return top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb06b9-f85e-4220-b0e6-2e130e965953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_query_l1(query_vector, data, k):\n",
    "    # calculate L1 distances\n",
    "    l1_distances = cdist(data, query_vector.reshape(1, -1), 'cityblock').flatten()\n",
    "\n",
    "    # get top-k indices\n",
    "    top_k_indices = np.argpartition(l1_distances, k)[:k]\n",
    "    \n",
    "    # return indices of the top-k closest vectors\n",
    "    return top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0491b633-c058-419d-9957-0322963e235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_query_l2(query_vector, data, k):\n",
    "    # calculate L2 distances\n",
    "    l1_distances = cdist(data, query_vector.reshape(1, -1), 'euclidean').flatten()\n",
    "\n",
    "    # get top-k indices\n",
    "    top_k_indices = np.argpartition(l1_distances, k)[:k]\n",
    "    \n",
    "    # return indices of the top-k closest vectors\n",
    "    return top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69188d6-deda-4ec1-999e-03fd4533cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(images, image_number, ri = 3, gi = 2, bi = 1):\n",
    "    # Check that image_number is valid\n",
    "    if image_number < 0 or image_number >= images.shape[0]:\n",
    "        raise ValueError('image_number must be between 0 and the number of images')\n",
    "\n",
    "    # Get the RGB bands (adjusting for 1-based indexing)\n",
    "    r = images[image_number, ri, :, :] # Red band\n",
    "    g = images[image_number, gi, :, :] # Green band\n",
    "    b = images[image_number, bi, :, :] # Blue band\n",
    "\n",
    "    # Stack them along the last dimension to create an RGB image\n",
    "    rgb = np.stack([r, g, b], axis=-1)\n",
    "\n",
    "    # Clamp and scale to [0, 255] range for display\n",
    "    rgb = np.clip(rgb, 0, 2500)  # Ensure values are within 0-2500\n",
    "    rgb = (rgb / 2500) * 255  # Scale values to 0-255\n",
    "\n",
    "    # Convert to 8-bit unsigned integer type\n",
    "    rgb = rgb.astype(np.uint8)\n",
    "\n",
    "    # Show the image\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(rgb)\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543e194a-5e38-4dd0-a660-9842f640bf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all_images(images, ri = 3, gi = 2, bi = 1):\n",
    "    # Determine the grid size to accommodate all images\n",
    "    grid_size = int(np.ceil(np.sqrt(images.shape[0])))\n",
    "\n",
    "    fig, ax = plt.subplots(grid_size, grid_size, figsize=(12, 12))\n",
    "\n",
    "    for i in range(grid_size * grid_size):\n",
    "        if i < images.shape[0]:\n",
    "            # Get the RGB bands (adjusting for 1-based indexing)\n",
    "            r = images[i, ri, :, :]  # Red band\n",
    "            g = images[i, gi, :, :]  # Green band\n",
    "            b = images[i, bi, :, :]  # Blue band\n",
    "\n",
    "            # Stack them along the last dimension to create an RGB image\n",
    "            rgb = np.stack([r, g, b], axis=-1)\n",
    "\n",
    "            # Clamp and scale to [0, 255] range for display\n",
    "            rgb = np.clip(rgb, 0, 2500)  # Ensure values are within 0-2500\n",
    "            rgb = (rgb / 2500) * 255  # Scale values to 0-255\n",
    "\n",
    "            # Convert to 8-bit unsigned integer type\n",
    "            rgb = rgb.astype(np.uint8)\n",
    "\n",
    "            # Display the image\n",
    "            ax[i // grid_size, i % grid_size].imshow(rgb)\n",
    "            ax[i // grid_size, i % grid_size].axis('off')  # Hide the axes\n",
    "        else:\n",
    "            # Hide empty subplots\n",
    "            ax[i // grid_size, i % grid_size].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bf7a34-d889-4b64-890f-6f877fb7f3d2",
   "metadata": {},
   "source": [
    "## Visual-visual queries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458b4b10-aa8c-4a8f-880d-f23c8ad55fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ri = 2\n",
    "gi = 1\n",
    "bi = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aed9c6e-fb5b-4427-9100-b2c379654b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "ri = 3\n",
    "gi = 2\n",
    "bi = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa033b7d-f339-40cc-b8cf-94ef13d27368",
   "metadata": {},
   "outputs": [],
   "source": [
    "center = torch.mean(_visual_embeddings, dim=0, keepdims=True)\n",
    "centered_visual_embeddings = F.normalize(_visual_embeddings - center, dim=1).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd3a126-966b-42ba-899f-ebef59174b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "center = center.detach().cpu().numpy()\n",
    "centered_visual_embeddings = centered_visual_embeddings.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b4a86e-92b0-4693-8631-f2b724ae8b36",
   "metadata": {},
   "source": [
    "### Water ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd2486-6c71-41c1-8a17-217b625a71b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = centered_visual_embeddings[440*2]\n",
    "print(top_k_query_cosine(query_vector, centered_visual_embeddings, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bfd271-8007-4cd4-927e-0ecb52451c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_this = ds[440*2][0]\n",
    "images_this = images_this.detach().cpu().numpy()\n",
    "display_all_images(images_this, ri, gi, bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84df3a22-f23a-44ae-9842-66a017c5bb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_neighbor = ds[872][0]\n",
    "images_neighbor = images_neighbor.detach().cpu().numpy()\n",
    "display_all_images(images_neighbor, ri, gi, bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f0e136-456c-4fb4-8769-0091866a20f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_neighbor = ds[874][0]\n",
    "images_neighbor = images_neighbor.detach().cpu().numpy()\n",
    "display_all_images(images_neighbor, ri, gi, bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd121a-01bd-4591-b977-e5f5e43ed13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_neighbor = ds[878][0]\n",
    "images_neighbor = images_neighbor.detach().cpu().numpy()\n",
    "display_all_images(images_neighbor, ri, gi, bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27682530-7f7c-41ad-8407-88b5c3dfaead",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_neighbor = ds[828][0]\n",
    "images_neighbor = images_neighbor.detach().cpu().numpy()\n",
    "display_all_images(images_neighbor, ri, gi, bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5860d669-5532-4bef-aa4f-467a08c30939",
   "metadata": {},
   "source": [
    "### Farmland(?) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6230bb3-3a3d-40f4-b098-2967ade85ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = centered_visual_embeddings[330*2]\n",
    "print(top_k_query_cosine(query_vector, centered_visual_embeddings, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8e2e64-9fd7-485e-83fc-d461ac431129",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_this = ds[330*2][0]\n",
    "images_this = images_this.detach().cpu().numpy()\n",
    "display_all_images(images_this, ri, gi, bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb717c16-cf26-48c0-866d-7d6dc5df37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_neighbor = ds[868][0]\n",
    "images_neighbor = images_neighbor.detach().cpu().numpy()\n",
    "display_all_images(images_neighbor, ri, gi, bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0ff9d0-410e-403f-9be1-f95d735d57ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_neighbor = ds[214][0]\n",
    "images_neighbor = images_neighbor.detach().cpu().numpy()\n",
    "display_all_images(images_neighbor, ri, gi, bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea523640-0427-4cf3-9b59-9d4bffce5580",
   "metadata": {},
   "source": [
    "### Buildings ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2faa74b-eda4-453b-b06d-2e7139ad2b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = centered_visual_embeddings[(19 + 3*21)*2]\n",
    "print(top_k_query_cosine(query_vector, centered_visual_embeddings, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a37d875-7f5f-44df-8fcb-05c665759a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_this = ds[(19 + 3*21)*2][0]\n",
    "images_this = images_this.detach().cpu().numpy()\n",
    "display_all_images(images_this, ri, gi, bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cedd47-f08e-46db-91c7-e87a6f441ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_neighbor = ds[248][0]\n",
    "images_neighbor = images_neighbor.detach().cpu().numpy()\n",
    "display_all_images(images_neighbor, ri, gi, bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fa90ad-d9e0-42d8-bbf9-b2a243e3ed12",
   "metadata": {},
   "source": [
    "## Text queries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f89ba1a-a762-4fbe-a356-fb2753c7257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "center = torch.mean(_stuff[0], dim=0, keepdims=True)\n",
    "centered_stuff = F.normalize(_stuff[0] - center, dim=1)\n",
    "center = center.detach().cpu()\n",
    "centered_stuff = centered_stuff.detach().cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90d179d-d579-4af6-b358-aa9fb08dd399",
   "metadata": {},
   "source": [
    "### Text similarity ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3af3673-61bc-4287-9c6f-43e424aea3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from InstructorEmbedding import INSTRUCTOR\n",
    "embed_model = INSTRUCTOR(\"hkunlp/instructor-xl\").to(device)\n",
    "embed_model.max_seq_length = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32eb2d3-0c53-48cf-b17b-258034c6922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_visual_query(query_text, instruction, center, embeddings, k: int = 5):\n",
    "    query = embed_model.encode([[instruction, query_text]])\n",
    "    query = torch.from_numpy(query).to(device)\n",
    "    with torch.inference_mode():\n",
    "        _, z = autoencoder.autoencoder_2(query)\n",
    "        z = z / z.norm(dim=1, keepdim=True)\n",
    "        query = autoencoder.autoencoder_1.decoder(z)\n",
    "        # query = query / query.norm(dim=1, keepdim=True)  # XXX\n",
    "    query = F.normalize(query - center.to(query.device), dim=1)\n",
    "    query.detach().cpu().numpy()\n",
    "    return top_k_query_cosine(query, embeddings, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522232fe-f9ab-4df7-88c8-396f10147a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Represent the geospatial data for retrieval; Input: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565e99b4-d542-4a13-b59f-5c2b9270d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_visual_query(\"Land use land cover: farmland.\", instruction, center, centered_stuff, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59785fc1-83f7-41da-852c-73a457484f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_visual_query(\"Buildings: ten.\", instruction, center, centered_stuff, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f63c5df-cc44-4aac-beb4-27b3a7a829f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_neighbor = ds[242][0]\n",
    "images_neighbor = images_neighbor.detach().cpu().numpy()\n",
    "display_all_images(images_neighbor, ri, gi, bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f75afc-ab3e-4dee-a12c-bb13a5eaba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(images_neighbor, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8919e53d-c804-4701-be59-7e1f52b1153b",
   "metadata": {},
   "source": [
    "### \"Zero shot\" classification ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf29356-ab13-403c-b2b6-15ad9d20da95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_percentage_bars(values, labels):\n",
    "    # Ensure values add up to 1\n",
    "    assert np.isclose(np.sum(values), 1), \"Values do not add up to 1\"\n",
    "\n",
    "    # Convert values to percentages\n",
    "    percentages = values * 100\n",
    "\n",
    "    # Create the bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(labels, percentages, color='skyblue')\n",
    "\n",
    "    # Add value labels on top of the bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, height, f'{height:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "    # Set title and ylabel\n",
    "    plt.title('Bar Graph with Percentage Values')\n",
    "    plt.ylabel('% Percentage')\n",
    "    plt.xticks(rotation=45, ha=\"right\") # Rotate x labels for better visibility\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f05191f-8b43-46dc-ac11-525e634042c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lulc_strings = [\n",
    "    [instruction, \"Land use land cover: agricultural.\"],\n",
    "    [instruction, \"Land use land cover: forest.\"],\n",
    "    [instruction, \"Land use land cover: water.\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad5f83-f00f-4fbf-9cbf-982577a60eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    lulcs = embed_model.encode(lulc_strings)\n",
    "    lulcs = torch.from_numpy(lulcs).to(device)\n",
    "    _, z = autoencoder.autoencoder_2(lulcs)\n",
    "    z = z / z.norm(dim=1, keepdim=True)\n",
    "    lulcs = autoencoder.autoencoder_1.decoder(z)\n",
    "    lulcs = lulcs - center.to(lulcs.device)\n",
    "    lulcs = F.normalize(lulcs, dim=1)\n",
    "    lulcs = lulcs.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e44668-71a0-41ab-b499-e73b6eaccb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "indxs = [80, 162, 224, 880]\n",
    "query = centered_stuff[indxs]\n",
    "query = query - center.to(query.device)\n",
    "query = F.normalize(query, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4b3b25-333f-4394-8cef-7125b76bcdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = F.softmax((query @ lulcs.t())/1e-1, dim=1)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d610b4ab-8006-494a-9d16-65f979086379",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d28298c-34a5-45c5-b056-4c36f617d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_neighbor = ds[indxs[i]][0]\n",
    "images_neighbor = images_neighbor.detach().cpu().numpy()\n",
    "display_all_images(images_neighbor, ri, gi, bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb1033d-5fd4-40b9-b03e-e83afbdf960b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_percentage_bars(results[i].numpy(), [s[1] for s in lulc_strings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d90a65-946b-484c-b35a-55467a86e5e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
